{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном ноутбуке по описаниям книг будут построены векторные предсатвления с помощью трансформеров для задачи Text Similarity. \\\n",
    "Источник модели: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "Построенные вектора будут использоваться для поиска похожих авторов / произведений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = pd.concat([pd.read_csv(str(x.resolve())) for x in Path(\"data/\").glob(\"*k.csv\")])\n",
    "\n",
    "columns = ['Id', 'Name', 'RatingDist1', 'RatingDist2', 'RatingDist3',\n",
    "           'RatingDist4', 'RatingDist5', 'Rating', 'RatingDistTotal', 'pagesNumber', 'Publisher',\n",
    "           'Authors', 'Language', 'Description']\n",
    "full_dataset = full_dataset[columns]\n",
    "\n",
    "full_dataset = full_dataset[~full_dataset['Description'].isna()]\n",
    "full_dataset['RatingDistTotal'] = full_dataset['RatingDistTotal'].apply(lambda x: float(x.replace('total:', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 113.93 Mb (15.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "full_dataset = reduce_mem_usage(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_preprocessing_step(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Удаляем html-теги\n",
    "    re_html = re.compile(r'<.*?>')\n",
    "    text = re_html.sub(' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Т.к. книг много, ограничим их кол-во, для экономии времени\n",
    "full_dataset = full_dataset[full_dataset['Description'].apply(len) >= 50]\n",
    "full_dataset = full_dataset[full_dataset['RatingDistTotal'] >= 2000]\n",
    "\n",
    "descriptions = full_dataset['Description'].apply(text_preprocessing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Строим ембединги, используя загруженную модель.\n",
    "embeddings = model.encode(descriptions.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можно загрузить уже посчитанный и сохранённый набор векторов.\n",
    "\n",
    "with open('result/embeddings_v1.pkl', 'rb') as file:\n",
    "    embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((66520, 384), (66520,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape, descriptions.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном датасете использовались несколько разных языков, оставим только те произведения где язык точно английский, и где он не указан."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21080\\3901544529.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfilters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Language'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'eng'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Language'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdescriptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfull_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "filters = (full_dataset['Language'] == 'eng') | (full_dataset['Language'].isna())\n",
    "\n",
    "embeddings = embeddings[filters]\n",
    "descriptions = descriptions[filters]\n",
    "full_dataset = full_dataset[filters]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждой книги посчитаем топ 20 ближайших к ней. Сохраним имена этих книг в словарь, который дальше будем использовать в telegram боте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53221it [13:04, 67.84it/s]\n"
     ]
    }
   ],
   "source": [
    "distances = []\n",
    "near_indexes = []\n",
    "\n",
    "for i, emb in tqdm(enumerate(embeddings)):\n",
    "    distances_tmp = np.dot(embeddings, emb)\n",
    "    indexes = np.argsort(distances_tmp)[::-1][:20]\n",
    "\n",
    "    distances.append(distances_tmp[indexes])\n",
    "    near_indexes.append(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53221it [00:22, 2315.19it/s]\n"
     ]
    }
   ],
   "source": [
    "books_dict = {}\n",
    "\n",
    "for i, idxs in tqdm(enumerate(near_indexes)):\n",
    "    book_name = full_dataset.iloc[i]['Name']\n",
    "    similar_books = full_dataset.iloc[idxs]['Name'].drop_duplicates().values\n",
    "    similar_books = [name for name in similar_books if name != book_name]\n",
    "\n",
    "    books_dict[book_name] = similar_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/book_distances_v1.pkl', 'wb') as file:\n",
    "    pickle.dump([distances, near_indexes], file)\n",
    "\n",
    "with open('result/books_dict_v1.pkl', 'wb') as file:\n",
    "    pickle.dump(books_dict, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее посчитаем вектор-представление каждого автора, как среднее векторов описаний его книг. \\\n",
    "Для каждого из них найдем топ 10 похожих авторов, сохраним их имена в виде слоавря, так же для telegram бота."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8422/8422 [00:56<00:00, 148.81it/s]\n"
     ]
    }
   ],
   "source": [
    "authors_embed = {}\n",
    "\n",
    "for author in tqdm(full_dataset['Authors'].unique()):\n",
    "    books_filter = full_dataset['Authors'] == author\n",
    "    mean_embed = embeddings[books_filter].mean(axis=0)\n",
    "\n",
    "    authors_embed[author] = mean_embed\n",
    "\n",
    "\n",
    "index_to_author = {i: key for i, key in enumerate(authors_embed.keys())}\n",
    "author_to_index = {key: i for i, key in enumerate(authors_embed.keys())}\n",
    "\n",
    "authors_matrix = np.array([embed for embed in authors_embed.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8422it [00:21, 391.60it/s]\n"
     ]
    }
   ],
   "source": [
    "authors_distances = []\n",
    "authors_near_indexes = []\n",
    "\n",
    "for i, emb in tqdm(enumerate(authors_matrix)):\n",
    "    distances_tmp = np.dot(authors_matrix, emb)\n",
    "    indexes = np.argsort(distances_tmp)[::-1][:10]\n",
    "\n",
    "    authors_distances.append(distances_tmp[indexes])\n",
    "    authors_near_indexes.append(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8422it [00:00, 85034.48it/s]\n"
     ]
    }
   ],
   "source": [
    "authors_dict = {}\n",
    "\n",
    "for i, idxs in tqdm(enumerate(authors_near_indexes)):\n",
    "    author_name = index_to_author[i]\n",
    "    similar_authors = [index_to_author[i] for i in idxs if index_to_author[i] != author_name]\n",
    "\n",
    "    authors_dict[author_name] = similar_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/authors_embed_v1.pkl', 'wb') as file:\n",
    "    pickle.dump(authors_embed, file)\n",
    "\n",
    "with open('result/authors_dict_v1.pkl', 'wb') as file:\n",
    "    pickle.dump(authors_dict, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим полученные результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tim Underwood',\n",
       " 'Al Sarrantonio',\n",
       " 'Robin Furth',\n",
       " 'Keith R.A. DeCandido',\n",
       " 'Lydia Davis',\n",
       " 'Alexander Masters',\n",
       " 'Benjamin Zephaniah',\n",
       " 'Steve Toltz',\n",
       " 'Douglas E. Winter',\n",
       " 'Graham McNamee']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_dict['Stephen King']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Newt Scamander',\n",
       " 'Marc Shapiro',\n",
       " 'Melissa Anelli',\n",
       " 'Roger Highfield',\n",
       " 'Suzy Kline',\n",
       " 'Camron Wright',\n",
       " 'Allan Zola Kronzek',\n",
       " 'David Colbert',\n",
       " 'M.V. Carey']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_dict['J.K. Rowling']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находятся авторы. которые участвовали и помогали исходным авторам \\\n",
    "Авторы, у которых книги похожи по именам/персонажам и т.п. \\\n",
    "Авторы, которые на самом деле по стилистике чем-то похожи \\\n",
    "Авторы приквелов, книгах, которые о данных книгах"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры авторов для проверки: \n",
    "\n",
    "Stephen King \\\n",
    "J.K. Rowling \\\n",
    "Arthur Conan Doyle \\\n",
    "Agatha Christie \\\n",
    "J.R.R. Tolkien \\\n",
    "Mark Twain \\\n",
    "William Shakespeare"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры книг для проверки:\n",
    "\n",
    "Calvin and Hobbes \\\n",
    "The Hobbit and The Lord of the Rings \\\n",
    "It \\\n",
    "The Green Mile \\\n",
    "Harry Potter and the Prisoner of Azkaban \\\n",
    "The Adventures of Tom Sawyer \\\n",
    "The Adventures of Sherlock Holmes \\\n",
    "Romeo And Juliet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Оценим качество получившихся эмбедингов для поиска похожих книг.\n",
    "Для этого возьмём небольшой датасет с оценками пользователей. \\\n",
    "Из них оставим только те книги, которые пользователи оценили положительно. \\\n",
    "Будем считать, что если две книги встретились у одного пользователя, то они условно \"близки\" друг к другу. \\\n",
    "В качестве метрики ранжирования возьмём усреднённый Precision@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating = pd.concat([pd.read_csv(str(x.resolve())) for x in Path(\"data/\").glob(\"user_*.csv\")])\n",
    "user_rating = pd.merge(user_rating, full_dataset[['Id', 'Name']], on='Name', suffixes=('', '_book'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/authors_dict_v1.pkl', 'rb') as file:\n",
    "    authors_dict = pickle.load(file)\n",
    "\n",
    "with open('result/books_dict_v1.pkl', 'rb') as file:\n",
    "    books_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Берём только позитивно оцененные книги\n",
    "user_rating_pos = user_rating[user_rating['Rating'].isin(['it was amazing', 'really liked it', 'liked it'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3734/3734 [02:08<00:00, 28.96it/s] \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Собираем книги, которые встречались вместе у пользователей.\n",
    "'''\n",
    "\n",
    "book_pairs = {}\n",
    "\n",
    "for user_id in tqdm(user_rating_pos['ID'].unique()):\n",
    "    user_books = user_rating_pos[user_rating_pos['ID'] == user_id]['Name'].values\n",
    "\n",
    "    for i, name1 in enumerate(user_books):\n",
    "        for j, name2 in enumerate(user_books, i+1):\n",
    "            \n",
    "            if name1 > name2:\n",
    "                pair_names = (name1, name2)\n",
    "            else:\n",
    "                pair_names = (name2, name1)\n",
    "\n",
    "            if book_pairs.get(pair_names) is None:\n",
    "                book_pairs[pair_names] = 1\n",
    "            else:\n",
    "                book_pairs[pair_names] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in user_rating_pos['Name'].unique():\n",
    "    del book_pairs[(name, name)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитаем среднее значение Precision@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем словарь с книгами, с совместной встречаемостью.\n",
    "\n",
    "single_book_dict = {}\n",
    "for name1, name2 in book_pairs.keys():\n",
    "\n",
    "    if single_book_dict.get(name1) is None:\n",
    "        single_book_dict[name1] = [name2]\n",
    "    else:\n",
    "        single_book_dict[name1].append(name2)\n",
    "\n",
    "    if single_book_dict.get(name2) is None:\n",
    "        single_book_dict[name2] = [name1]\n",
    "    else:\n",
    "        single_book_dict[name2].append(name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Restaurant at the End of the Universe (Hitchhiker's Guide to the Galaxy, #2)\",\n",
       " 'Siddhartha',\n",
       " 'The Hunger Games (The Hunger Games, #1)',\n",
       " 'The Authoritative Calvin and Hobbes: A Calvin and Hobbes Treasury',\n",
       " 'The Return of the Indian (The Indian in the Cupboard, #2)',\n",
       " 'The Name of the Rose',\n",
       " 'Dark Apprentice (Star Wars: The Jedi Academy Trilogy, #2)',\n",
       " 'A Short History of Nearly Everything',\n",
       " 'Angels & Demons (Robert Langdon, #1)',\n",
       " 'The Return of the King (The Lord of the Rings, #3)']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_book_dict['Freakonomics: A Rogue Economist Explores the Hidden Side of Everything'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36519/36519 [00:00<00:00, 47491.58it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Посчитаем средний Precision@10\n",
    "'''\n",
    "\n",
    "precision_sum = 0\n",
    "precision_cnt = 0\n",
    "\n",
    "for name, sim_books in tqdm(books_dict.items()):\n",
    "    if single_book_dict.get(name) is None:\n",
    "        continue\n",
    "    \n",
    "    K = min(10, len(single_book_dict[name]))\n",
    "    intersetction = len(set(single_book_dict[name]).intersection(set(sim_books[:K])))\n",
    "\n",
    "    precision_sum += float(intersetction) / K\n",
    "    precision_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение Precision@10 = 0.128\n"
     ]
    }
   ],
   "source": [
    "print(f'Среднее значение Precision@10 = {round(precision_sum / precision_cnt, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нечёткий поиск для опечаток в telegrem боте.\n",
    "Возможна ситуация, когда пользователь телеграм бота неправильно ввёл название книги или имя автора. \\\n",
    "В этом случае воспользуемся библиотекой RapidFuzz https://github.com/maxbachmann/RapidFuzz для поиска похожих наименований. \\\n",
    "Данная библиотека позволяет очень быстро находить матчи. Что достаточно важно для пользовательского опыта \\\n",
    "Встроим эту систему в ТГ бот."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36519, 8422)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_names = list(books_dict.keys())\n",
    "authors_names = list(authors_dict.keys())\n",
    "\n",
    "len(books_names), len(authors_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269 ms ± 3.59 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "query = 'Haroun and Sea stories'.lower()\n",
    "res = process.extract(query, books_names, limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Haroun and the Sea of Stories', 95.0, 0),\n",
       " (\"There and Back Again: An Actor's Tale\", 85.5, 1),\n",
       " ('Rush Limbaugh Is a Big Fat Idiot and Other Observations (Americana)',\n",
       "  85.5,\n",
       "  26),\n",
       " ('Chinese Cinderella and the Secret Dragon Society: By the Author of Chinese Cinderella',\n",
       "  85.5,\n",
       "  76),\n",
       " (\"The Girls' Guide to Hunting and Fishing\", 85.5, 80)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0d5d6461784772888fb12206aba4f7d242d28ab549b12d9512b024fa08541f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
